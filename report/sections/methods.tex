\section{Methods}%
\label{sec:methods}

\subsection{Entity Embeddings}%
\label{sub:entity_embeddings}

We map categorical variables with $C$ categories represented by indices $c \in
\{ 0,1,\dots,C-1\}$ to real-numbered vectors $\vec{x}_c \in \R^n$
\begin{equation}
    \Embed: \{0,1,\dots,C-1\} \rightarrow \R^n, c \mapsto \Embed(c) = \vec{x}_c.
\end{equation}
These embedding layers are implemented as lookup tables. The vector associated
with each index is a parameter of the model and is learnd jointly with the rest
of the model.

If the input to our model is a mixture of continuous and categorical variables
as is the case here, we learn one embedding layer for each of the categorical
variables and concatenate the vector components of each embedding output
together with the continuous variables to one vector. This concatenated vector
then serves as the input to the rest of the model.

\subsection{Neural Networks}%
\label{sub:neural_networks}

\begin{figure}[htb!]
    \def\layersep{2.5cm}
    \centering
    \begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
        \tikzstyle{every pin edge}=[<-,shorten <=1pt]
        \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
        \tikzstyle{input neuron}=[neuron, fill=green!50];
        \tikzstyle{output neuron}=[neuron, fill=red!50];
        \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
        \tikzstyle{annot} = [text width=4em, text centered]

        % Draw the input layer nodes
        \foreach \name / \y in {1,...,4}
        % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:Input \#\y] (I-\name) at (0,-\y) {};

        % Draw the hidden layer nodes
        \foreach \name / \y in {1,...,5}
        \path[yshift=0.5cm]
            node[hidden neuron] (H-\name) at (\layersep,-\y cm) {};

        % Draw the output layer node
        \node[output neuron,pin={[pin edge={->}]right:Output}, right of=H-3] (O) {};

        % Connect every node in the input layer with every node in the
        % hidden layer.
        \foreach \source in {1,...,4}
        \foreach \dest in {1,...,5}
        \path (I-\source) edge (H-\dest);

        % Connect every node in the hidden layer with the output layer
        \foreach \source in {1,...,5}
        \path (H-\source) edge (O);

        % Annotate the layers
        \node[annot,above of=H-1, node distance=1cm] (hl) {Hidden layer};
        \node[annot,left of=hl] {Input layer};
        \node[annot,right of=hl] {Output layer};
    \end{tikzpicture}
    \caption{2-Layer neural network}
    \label{fig:neural_network}
\end{figure}

Feed-forward neural networks, sometimes called multilayer perceptrons, are one
of many machine learning models designed to approximate some function $f^*$.
They define a mapping $y=f(x,\theta)$ where $\theta$ is learned to result in
the best approximation.

The name feed-forward neural network stems from the
fact that they consist of intermediary functions called layers, that are
chained together. The length of the chain of these intermediary functions gives
the depth of the network. As $f^*$, $f$ and all intermediary functions are
vertor valued, the dimensionality of the vector gives the width of the layer.
If a layer is not the input or output layer it is called hidden.
By depicting each vector component as a node, neural networks can be described
by directed acyclic graphs as in \autoref{fig:neural_network}~\cite{Goodfellow-et-al-2016}.

In most cases, each layer consists of a linear function $y=w^T x$, where $w$
are called the weights of that layer and are part of $\theta$. As a chain of
linear functions is still a linear function, we need something to make the
neural network nonlinear to learn general functions. To acomplish this, each
layer is associated with a nonlinear function called the activation function,
that is applied to the result of the linear function. Popular choices for
activation functions are the rectified linear unit (ReLU)
\begin{equation}
    \ReLU(x)=\max(0, x)
\end{equation}
and the softmax~\cite{Goodfellow-et-al-2016}. %TODO

\subsection{Gaussian Processes}%
\label{sub:gaussian_processes}

If we want to estimate some unobserved function $f = f(\vec{x})$ responsible
for generating some set of observed variables $\mathbf{Y} \in \R^{N}$
from a corresponding set of input variables $\mathbf{X} \in \R^{N \times D}$,
Gaussian Processes can be used to obtain nonparametric prior distributions over
the latent function $f$.

Formally, each datapoint $y_n$ is generated from $f(\vec{x}_n)$ by adding
Gaussian noise
\begin{equation}
y_n = f(\vec{x}_n) + \epsilon_n, \quad
\epsilon_n \sim \mathcal{N}(0,\sigma_{\epsilon}^2\vec{I})
\end{equation}
and $f$ is drawn from a zero-mean Gaussian process
\begin{equation}
f \sim \mathcal{GP}(\vec{0},k(x,x'))
\end{equation}
defined by its covariance function $k$ operating on the inputs $\vec{X}$. To
obtain a flexible model only very general assumptions are made when choosing
the covariance function. The popular radial basis function $k(x, x')=\sigma^2
\exp^{-\frac{(x-x')^2}{2l^2}}$ for instance only makes an assumption about the
smoothness of $f$. We denote kernel function hyperparameters as
$\vec{\theta}$.
The marginal likelihood for a set of outputs given a set of inputs can now be analytically computed as~\cite{damianou2013deep}
\begin{equation}
    p(\vec{Y} | \vec{X}) = \mathcal{N}(\vec{Y}|\vec{0}, k(\vec{X}, \vec{X}) +
    \sigma_{\epsilon}\mathbb{1}).
\end{equation}

If want now to predict the output $y^*$ for some novel input $\vec{x}^*$, we have to
condition the joint distribution of the outputs
\begin{equation}
    p(\vec{Y}, y^*|\vec{X}, \vec{x}^*) = \mathcal{N}(\vec{Y}, y^*|\vec{0},
    \vec{K}^+), \quad \vec{K}^+ =
    \begin{pmatrix}
        \vec{K}_{\vec{X}\vec{X}} & \vec{K}_{\vec{X}\vec{x}^*} \\
        \vec{K}_{\vec{x}^*\vec{X}} & \vec{K}_{\vec{x}^*\vec{x}^*} \\
    \end{pmatrix}
\end{equation}
on the known outputs to obtain~\cite{barberBRML2012}
\begin{equation}
    p(y^* | \vec{x}^*, \vec{X}, \vec{Y}) = \mathcal{N}(y^* |\ \vec{K}_{\vec{x}^*\vec{X}}
    \vec{K}_{\vec{X}\vec{X}}^{-1}\vec{Y},\ \vec{K}_{\vec{x}^*\vec{x}^*} -
    \vec{K}_{\vec{x}^*\vec{X}} \vec{K}_{\vec{X}\vec{X}}^{-1} \vec{K}_{\vec{X}\vec{x}^*}).
\end{equation}
This computation, however, has complexity $\mathcal{O}(N^3)$.

To be able to use Gaussian processes for large datasets we apply stochastic
variational inference to the model. For this we introduce a set of inducing
variables $\vec{U}$ that represent the values of $f$ at the points $\vec{Z} \in
\R^{M \times D}$ living in $\vec{X}$ space, where $M<N$. Predictions then take
the form
\begin{equation}
    p(y^* | \vec{x}^*, \vec{Z}, \vec{U}) = \mathcal{N}( y^* |
    \vec{K}_{\vec{x}^*\vec{Z}} \vec{K}_{\vec{Z}\vec{Z}}^{-1}\vec{U},\
    \vec{K}_{\vec{x}^*\vec{x^*}} - \vec{K}_{\vec{x}^*\vec{Z}}
    \vec{K}_{\vec{Z}\vec{Z}}^{-1}\vec{K}_{\vec{Z}\vec{x}^*})
\end{equation}

We estimate the inducing points $\vec{Z}$ using stochastic variational
interference by using a variational distribution $q(\vec{U}) = \mathcal{N}
(\vec{U} | \vec{m}, \vec{S})$ to
place a lower bound on $p(\vec{Y} | \vec{X})$:
\begin{equation}
    \log p(\vec{Y} | \vec{X}) \geq \sum_{i=1}^{N} \mathcal{L}_i -
    \KL(q(\vec{U}) || p(\vec{U}))
\end{equation}
where $\mathcal{L}_i$ only depends on one input-output pair ${\vec{x}_i, y_i}$.
The derivation and exact definition of $\mathcal{L}$ in out of scope for this
report and not importantn. It can be found in~\cite{hensman2013gaussian}. The
important property of this lower bound is that it is a sum of terms where each
only corresponds to one input-output pair, allowing us to use stochastic
gradient descent to train the model.

